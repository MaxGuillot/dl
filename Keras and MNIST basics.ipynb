{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Purpose</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-handling\" data-toc-modified-id=\"Data-handling-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data handling</a></span></li><li><span><a href=\"#Hyperparameters-definition\" data-toc-modified-id=\"Hyperparameters-definition-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Hyperparameters definition</a></span></li></ul></li><li><span><a href=\"#Model-construction:-towards-a-MLP\" data-toc-modified-id=\"Model-construction:-towards-a-MLP-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model construction: towards a MLP</a></span></li><li><span><a href=\"#Todo\" data-toc-modified-id=\"Todo-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Todo</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "Well, this notebook is intended to practice the basics of Keras with [MNIST](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data handling\n",
    "Keras library comes with datasets loader built-in, so let us enjoy the pleasure !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(str(X_train.shape) + \" \"+ str(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X format is a 3-d matrix, what we do not wish to mingle with. Let's shape it as a 2-D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(60000,28*28)\n",
    "X_test = X_test.reshape(10000,28*28)\n",
    "print(str(X_train.shape) + \" \"+ str(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since high values in input inhibits the network convergence, a safe step would be to divide every input vector by the known maximum of said array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n",
      "1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(X_train),np.max(X_train))\n",
    "X_train = np.divide(X_train,[np.max(X_train)])\n",
    "X_test = np.divide(X_test,[np.max(X_test)])\n",
    "print(np.max(X_train),np.max(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this a classification problem, one needs to make the target into categorical. Since there are ten digits possible, target will be splitted into ten booleans fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters definition\n",
    "Usually, MLP requires to know the dimensions of input and input, since it will decide the number of neurons in these layers.\n",
    "Next parameters are the number of epochs, batch size for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_input : 784 || nb_classes : 10\n"
     ]
    }
   ],
   "source": [
    "nb_input = X_train.shape[1]\n",
    "nb_classes = y_train.shape[1]\n",
    "print(\"nb_input : \"+ str(nb_input) + \" || nb_classes : \" + str(nb_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Model construction: towards a MLP\n",
    "Multi-Layer-Perceptron (MLP) is an architecture often referred as shallow feed-forward-network. It is capable of classifying efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential consists in a succession of layers, where every neuron of previous layer is conected to every neuron of the next layer _dense_.\n",
    "\n",
    "_relu_ activation enables fast computations, and in deeper networks, minimize information loss during retropropagation, _softmax_ enables a soft landing on the various classes.\n",
    "\n",
    "_0.2_ dropout allows a better redistribution of error to neurons by randomly shutting down some of them at each iteration.\n",
    "\n",
    "Input shape is defined by the number of variables for one sample, __784__, and the output has to be the same as the number of classes, __10__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(nb_input,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2047 - acc: 0.4910 - val_loss: 8.1590 - val_acc: 0.4938\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2602 - acc: 0.4875 - val_loss: 8.2380 - val_acc: 0.4889\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2277 - acc: 0.4895 - val_loss: 8.1254 - val_acc: 0.4958\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.2243 - acc: 0.4897 - val_loss: 8.1832 - val_acc: 0.4923\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2151 - acc: 0.4903 - val_loss: 8.1799 - val_acc: 0.4925\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2666 - acc: 0.4871 - val_loss: 8.1916 - val_acc: 0.4917\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.2286 - acc: 0.4895 - val_loss: 8.1687 - val_acc: 0.4932\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.2687 - acc: 0.4869 - val_loss: 8.1737 - val_acc: 0.4928\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.3108 - acc: 0.4844 - val_loss: 8.2567 - val_acc: 0.4876\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.2005 - acc: 0.4912 - val_loss: 8.1182 - val_acc: 0.4963\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2468 - acc: 0.4883 - val_loss: 8.1509 - val_acc: 0.4943\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2441 - acc: 0.4885 - val_loss: 8.2335 - val_acc: 0.4891\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.2630 - acc: 0.4873 - val_loss: 8.2073 - val_acc: 0.4908\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.3579 - acc: 0.4814 - val_loss: 8.2718 - val_acc: 0.4868\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2494 - acc: 0.4882 - val_loss: 8.1848 - val_acc: 0.4922\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2194 - acc: 0.4900 - val_loss: 8.1606 - val_acc: 0.4937\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.2001 - acc: 0.4912 - val_loss: 8.1332 - val_acc: 0.4954\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2372 - acc: 0.4889 - val_loss: 8.2718 - val_acc: 0.4868\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.2468 - acc: 0.4883 - val_loss: 8.2170 - val_acc: 0.4902\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2229 - acc: 0.4898 - val_loss: 8.1839 - val_acc: 0.4922\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2223 - acc: 0.4898 - val_loss: 8.1735 - val_acc: 0.4929\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2266 - acc: 0.4896 - val_loss: 8.1944 - val_acc: 0.4916\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2294 - acc: 0.4894 - val_loss: 8.1847 - val_acc: 0.4922\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.2219 - acc: 0.4899 - val_loss: 8.3588 - val_acc: 0.4814\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 12s - loss: 8.2507 - acc: 0.4881 - val_loss: 8.1471 - val_acc: 0.4945\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 12s - loss: 8.2069 - acc: 0.4908 - val_loss: 8.1332 - val_acc: 0.4954\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2737 - acc: 0.4866 - val_loss: 8.1815 - val_acc: 0.4924\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.1955 - acc: 0.4915 - val_loss: 8.1832 - val_acc: 0.4923\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2627 - acc: 0.4874 - val_loss: 8.2363 - val_acc: 0.4890\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2266 - acc: 0.4896 - val_loss: 8.1698 - val_acc: 0.4931\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2073 - acc: 0.4908 - val_loss: 8.2331 - val_acc: 0.4892\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2135 - acc: 0.4904 - val_loss: 8.3153 - val_acc: 0.4841\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 12s - loss: 8.1984 - acc: 0.4913 - val_loss: 8.1235 - val_acc: 0.4960\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.1872 - acc: 0.4920 - val_loss: 8.1558 - val_acc: 0.4940\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2079 - acc: 0.4907 - val_loss: 8.2138 - val_acc: 0.4904\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 12s - loss: 8.2043 - acc: 0.4910 - val_loss: 8.1461 - val_acc: 0.4946\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 12s - loss: 8.1865 - acc: 0.4921 - val_loss: 8.1522 - val_acc: 0.4942\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 12s - loss: 8.1973 - acc: 0.4914 - val_loss: 8.1714 - val_acc: 0.4930\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2207 - acc: 0.4900 - val_loss: 8.1869 - val_acc: 0.4920\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.2850 - acc: 0.4859 - val_loss: 8.2106 - val_acc: 0.4906\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2361 - acc: 0.4890 - val_loss: 8.1454 - val_acc: 0.4946\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2002 - acc: 0.4912 - val_loss: 8.1477 - val_acc: 0.4945\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2455 - acc: 0.4883 - val_loss: 8.2750 - val_acc: 0.4866\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2892 - acc: 0.4857 - val_loss: 8.2315 - val_acc: 0.4893\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2187 - acc: 0.4901 - val_loss: 8.1735 - val_acc: 0.4929\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2155 - acc: 0.4903 - val_loss: 8.2218 - val_acc: 0.4899\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2668 - acc: 0.4871 - val_loss: 8.3058 - val_acc: 0.4846\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2618 - acc: 0.4874 - val_loss: 8.1348 - val_acc: 0.4953\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2205 - acc: 0.4900 - val_loss: 8.1767 - val_acc: 0.4927\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.3366 - acc: 0.4828 - val_loss: 8.3008 - val_acc: 0.4850\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 12s - loss: 8.2480 - acc: 0.4882 - val_loss: 8.1300 - val_acc: 0.4956\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.1978 - acc: 0.4914 - val_loss: 8.1493 - val_acc: 0.4944\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2072 - acc: 0.4908 - val_loss: 8.1211 - val_acc: 0.4961\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2207 - acc: 0.4900 - val_loss: 8.1680 - val_acc: 0.4932\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.2484 - acc: 0.4882 - val_loss: 8.1933 - val_acc: 0.4916\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 12s - loss: 8.2186 - acc: 0.4901 - val_loss: 8.1433 - val_acc: 0.4947\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 12s - loss: 8.2407 - acc: 0.4887 - val_loss: 8.2405 - val_acc: 0.4887\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2935 - acc: 0.4854 - val_loss: 8.1881 - val_acc: 0.4918\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2224 - acc: 0.4899 - val_loss: 8.1670 - val_acc: 0.4933\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 12s - loss: 8.2521 - acc: 0.4880 - val_loss: 8.1235 - val_acc: 0.4960\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.1966 - acc: 0.4915 - val_loss: 8.1413 - val_acc: 0.4949\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 13s - loss: 8.2057 - acc: 0.4909 - val_loss: 8.1171 - val_acc: 0.4964\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 14s - loss: 8.1940 - acc: 0.4916 - val_loss: 8.1525 - val_acc: 0.4942\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 11s - loss: 8.1924 - acc: 0.4917 - val_loss: 8.1533 - val_acc: 0.4941\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 11s - loss: 8.2455 - acc: 0.4884 - val_loss: 8.1332 - val_acc: 0.4954\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 11s - loss: 8.3412 - acc: 0.4825 - val_loss: 8.3612 - val_acc: 0.4812\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 11s - loss: 8.2895 - acc: 0.4857 - val_loss: 8.1267 - val_acc: 0.4958\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 16s - loss: 8.1723 - acc: 0.4929 - val_loss: 8.1251 - val_acc: 0.4959\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 2806s - loss: 8.2013 - acc: 0.4912 - val_loss: 8.1574 - val_acc: 0.4939\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 20s - loss: 8.1904 - acc: 0.4918 - val_loss: 8.1316 - val_acc: 0.4955\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 18s - loss: 8.2440 - acc: 0.4885 - val_loss: 8.1267 - val_acc: 0.4958\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 17s - loss: 8.1935 - acc: 0.4916 - val_loss: 8.1670 - val_acc: 0.4933\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 16s - loss: 8.2935 - acc: 0.4854 - val_loss: 8.1493 - val_acc: 0.4944\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.1760 - acc: 0.4927 - val_loss: 8.1396 - val_acc: 0.4950\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.1782 - acc: 0.4926 - val_loss: 8.1362 - val_acc: 0.4952\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.1979 - acc: 0.4913 - val_loss: 8.1574 - val_acc: 0.4939\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 17s - loss: 8.2365 - acc: 0.4890 - val_loss: 8.1725 - val_acc: 0.4929\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.1825 - acc: 0.4923 - val_loss: 8.1509 - val_acc: 0.4943\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2069 - acc: 0.4908 - val_loss: 8.1815 - val_acc: 0.4924\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 16s - loss: 8.2378 - acc: 0.4889 - val_loss: 8.1528 - val_acc: 0.4941\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 16s - loss: 8.2213 - acc: 0.4899 - val_loss: 8.1396 - val_acc: 0.4950\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.2022 - acc: 0.4911 - val_loss: 8.1590 - val_acc: 0.4938\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.2389 - acc: 0.4888 - val_loss: 8.1706 - val_acc: 0.4930\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.1996 - acc: 0.4913 - val_loss: 8.2041 - val_acc: 0.4910\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.2487 - acc: 0.4882 - val_loss: 8.1271 - val_acc: 0.4957\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2420 - acc: 0.4886 - val_loss: 8.2525 - val_acc: 0.4880\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2596 - acc: 0.4875 - val_loss: 8.2138 - val_acc: 0.4904\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.3046 - acc: 0.4847 - val_loss: 8.2777 - val_acc: 0.4864\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2383 - acc: 0.4889 - val_loss: 8.1864 - val_acc: 0.4921\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.3256 - acc: 0.4834 - val_loss: 8.2057 - val_acc: 0.4909\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 17s - loss: 8.2147 - acc: 0.4903 - val_loss: 8.1282 - val_acc: 0.4957\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 17s - loss: 8.2101 - acc: 0.4906 - val_loss: 8.1815 - val_acc: 0.4924\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 16s - loss: 8.2048 - acc: 0.4909 - val_loss: 8.1474 - val_acc: 0.4945\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.3745 - acc: 0.4804 - val_loss: 8.2670 - val_acc: 0.4871\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.2019 - acc: 0.4911 - val_loss: 8.1525 - val_acc: 0.4942\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2116 - acc: 0.4905 - val_loss: 8.2639 - val_acc: 0.4872\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 16s - loss: 8.2157 - acc: 0.4902 - val_loss: 8.1912 - val_acc: 0.4918\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 14s - loss: 8.1891 - acc: 0.4919 - val_loss: 8.1477 - val_acc: 0.4945\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.1874 - acc: 0.4920 - val_loss: 8.1572 - val_acc: 0.4939\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 15s - loss: 8.2644 - acc: 0.4873 - val_loss: 8.1348 - val_acc: 0.4953\n",
      "Test loss: 8.13480284729\n",
      "Test accuracy: 0.4953\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP are usually sensitive to initialisation: in this case, bad luck.\n",
    "# Todo\n",
    "* Relaunch\n",
    "* Tune hyperparameters more\n",
    "* Get some illustration\n",
    "* ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "491px",
    "left": "0px",
    "right": "909.545px",
    "top": "107px",
    "width": "260px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
