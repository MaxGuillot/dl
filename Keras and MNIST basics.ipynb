{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Purpose</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-handling\" data-toc-modified-id=\"Data-handling-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data handling</a></span></li><li><span><a href=\"#Hyperparameters-definition\" data-toc-modified-id=\"Hyperparameters-definition-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Hyperparameters definition</a></span></li></ul></li><li><span><a href=\"#Model-construction:-towards-a-MLP\" data-toc-modified-id=\"Model-construction:-towards-a-MLP-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model construction: towards a MLP</a></span></li><li><span><a href=\"#Todo\" data-toc-modified-id=\"Todo-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Todo</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "Well, this notebook is intended to practice the basics of Keras with [MNIST](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data handling\n",
    "Keras library comes with datasets loader built-in, so let us enjoy the pleasure !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(str(X_train.shape) + \" \"+ str(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X format is a 3-d matrix, what we do not wish to mingle with. Let's shape it as a 2-D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(60000,28*28)\n",
    "X_test = X_test.reshape(10000,28*28)\n",
    "print(str(X_train.shape) + \" \"+ str(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this a classification problem, one needs to make the target into categorical. Since there are ten digits possible, target will be splitted into ten booleans fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters definition\n",
    "Usually, MLP requires to know the dimensions of input and input, since it will decide the number of neurons in these layers.\n",
    "Next parameters are the number of epochs, batch size for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_input : 784 || nb_classes : 10\n"
     ]
    }
   ],
   "source": [
    "nb_input = X_train.shape[1]\n",
    "nb_classes = y_train.shape[1]\n",
    "print(\"nb_input : \"+ str(nb_input) + \" || nb_classes : \" + str(nb_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Model construction: towards a MLP\n",
    "Multi-Layer-Perceptron (MLP) is an architecture often referred as shallow feed-forward-network. It is capable of classifying efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential consists in a succession of layers, where every neuron of previous layer is conected to every neuron of the next layer _dense_.\n",
    "\n",
    "_relu_ activation enables fast computations, and in deeper networks, minimize information loss during retropropagation, _softmax_ enables a soft landing on the various classes.\n",
    "\n",
    "_0.2_ dropout allows a better redistribution of error to neurons by randomly shutting down some of them at each iteration.\n",
    "\n",
    "Input shape is defined by the number of variables for one sample, __784__, and the output has to be the same as the number of classes, __10__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(nb_input,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 12s - loss: 14.6583 - acc: 0.0905 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 12s - loss: 14.6618 - acc: 0.0904 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 12s - loss: 14.6618 - acc: 0.0903 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 12s - loss: 14.6618 - acc: 0.0903 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 12s - loss: 14.6618 - acc: 0.0903 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 12s - loss: 14.6618 - acc: 0.0904 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 12s - loss: 14.6618 - acc: 0.0904 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 13s - loss: 14.6618 - acc: 0.0904 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 12s - loss: 14.6618 - acc: 0.0903 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 13s - loss: 14.6618 - acc: 0.0904 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 13s - loss: 14.6618 - acc: 0.0904 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 13s - loss: 14.6618 - acc: 0.0903 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 13s - loss: 14.6618 - acc: 0.0903 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 13s - loss: 14.6618 - acc: 0.0904 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 13s - loss: 14.6618 - acc: 0.0903 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 13s - loss: 14.6618 - acc: 0.0903 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 12s - loss: 14.6618 - acc: 0.0903 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 13s - loss: 14.6618 - acc: 0.0903 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 13s - loss: 14.6618 - acc: 0.0903 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 13s - loss: 14.6618 - acc: 0.0903 - val_loss: 14.6804 - val_acc: 0.0892\n",
      "Test loss: 14.6803610641\n",
      "Test accuracy: 0.0892\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP are usually sensitive to initialisation: in this case, bad luck.\n",
    "# Todo\n",
    "* Relaunch\n",
    "* Tune hyperparameters more\n",
    "* Get some illustration\n",
    "* ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "491px",
    "left": "0px",
    "right": "909.545px",
    "top": "107px",
    "width": "260px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
