{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Purpose</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-handling\" data-toc-modified-id=\"Data-handling-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data handling</a></span></li><li><span><a href=\"#Hyperparameters-definition\" data-toc-modified-id=\"Hyperparameters-definition-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Hyperparameters definition</a></span></li></ul></li><li><span><a href=\"#Model-construction:-towards-a-MLP\" data-toc-modified-id=\"Model-construction:-towards-a-MLP-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model construction: towards a MLP</a></span></li><li><span><a href=\"#Todo\" data-toc-modified-id=\"Todo-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Todo</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "Well, this notebook is intended to practice the basics of Keras with [MNIST](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data handling\n",
    "Keras library comes with datasets loader built-in, so let us enjoy the pleasure !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(str(X_train.shape) + \" \"+ str(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X format is a 3-d matrix, what we do not wish to mingle with. Let's shape it as a 2-D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(60000,28*28)\n",
    "X_test = X_test.reshape(10000,28*28)\n",
    "print(str(X_train.shape) + \" \"+ str(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since high values in input inhibits the network convergence, a safe step would be to divide every input vector by the known maximum of said array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n",
      "1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(X_train),np.max(X_train))\n",
    "X_train = np.divide(X_train,[np.max(X_train)])\n",
    "X_test = np.divide(X_test,[np.max(X_test)])\n",
    "print(np.max(X_train),np.max(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make this a classification problem, one needs to make the target into categorical. Since there are ten digits possible, target will be splitted into ten booleans fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters definition\n",
    "Usually, MLP requires to know the dimensions of input and input, since it will decide the number of neurons in these layers.\n",
    "Next parameters are the number of epochs, batch size for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_input : 784 || nb_classes : 10\n"
     ]
    }
   ],
   "source": [
    "nb_input = X_train.shape[1]\n",
    "nb_classes = y_train.shape[1]\n",
    "print(\"nb_input : \"+ str(nb_input) + \" || nb_classes : \" + str(nb_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Model construction: towards a MLP\n",
    "Multi-Layer-Perceptron (MLP) is an architecture often referred as shallow feed-forward-network. It is capable of classifying efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential consists in a succession of layers, where every neuron of previous layer is conected to every neuron of the next layer _dense_.\n",
    "\n",
    "_relu_ activation enables fast computations, and in deeper networks, minimize information loss during retropropagation, _softmax_ enables a soft landing on the various classes.\n",
    "\n",
    "_0.2_ dropout allows a better redistribution of error to neurons by randomly shutting down some of them at each iteration.\n",
    "\n",
    "Input shape is defined by the number of variables for one sample, __784__, and the output has to be the same as the number of classes, __10__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(nb_input,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 17s - loss: 0.2451 - acc: 0.9259 - val_loss: 0.1019 - val_acc: 0.9681\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 17s - loss: 0.1023 - acc: 0.9688 - val_loss: 0.0812 - val_acc: 0.9754\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 17s - loss: 0.0744 - acc: 0.9780 - val_loss: 0.0842 - val_acc: 0.9755\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 17s - loss: 0.0606 - acc: 0.9822 - val_loss: 0.0752 - val_acc: 0.9790\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 17s - loss: 0.0503 - acc: 0.9852 - val_loss: 0.0725 - val_acc: 0.9817\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 17s - loss: 0.0452 - acc: 0.9866 - val_loss: 0.0651 - val_acc: 0.9830\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 17s - loss: 0.0391 - acc: 0.9881 - val_loss: 0.0795 - val_acc: 0.9815\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 17s - loss: 0.0338 - acc: 0.9902 - val_loss: 0.0955 - val_acc: 0.9783\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 17s - loss: 0.0321 - acc: 0.9906 - val_loss: 0.0823 - val_acc: 0.9817\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 18s - loss: 0.0289 - acc: 0.9918 - val_loss: 0.0966 - val_acc: 0.9798\n",
      "Test loss: 0.0966141672839\n",
      "Test accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP are usually sensitive to initialisation and input values being between 0 and 1. After dividing the input arays by their maximum, being 255, the train accuracy jumped from 20-50% after 1000 epochs to 98% after 5 epochs.\n",
    "\n",
    "As of the test, 97% is a decent score, especially when no tuning has been done.\n",
    "# Todo\n",
    "* Tune hyperparameters more\n",
    "* Get some illustration\n",
    "* ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "491px",
    "left": "0px",
    "right": "909.545px",
    "top": "107px",
    "width": "260px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
